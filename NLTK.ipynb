{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (Natural Language Toolkit) and working with unstructured (text) data\n",
    "\n",
    "This notebook will introduce you to the Natural Language Toolkit, better known as NLTK (https://www.nltk.org/). \n",
    "NLTK is a leading platform for building Python programs to work with human language data.\n",
    "This type of work is called Natural Language Processing (NLP) and is a complex field.\n",
    "We can only provide the basics, but should give you a start if you want to work with text data.\n",
    "\n",
    "**Warning** NLTK can have a steep learning curve. They have introductory materials and other materials that you may find useful listed below.\n",
    "\n",
    " * NLTK Book\n",
    "   * http://www.nltk.org/book/\n",
    " * PythonProgramming tutorial series (with videos, quizzes, and many components)\n",
    "   * https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n",
    " * Reasonable walkthroughs\n",
    "   * https://likegeeks.com/nlp-tutorial-using-python-nltk/\n",
    "   * https://www.guru99.com/stemming-lemmatization-python-nltk.html\n",
    " * If you want another advanced package look at gensim (topic modeling)\n",
    "   * https://radimrehurek.com/gensim/\n",
    "\n",
    "A general NLP pipeline can be found in chapter 3 of the NLTK book. See image below for an example using HTML webpages.\n",
    "<img src=\"http://www.nltk.org/images/pipeline1.png\"/>\n",
    "\n",
    "In this notebook we setup a 3 stage pipeline:\n",
    "1. open and read data\n",
    "2. clean up data\n",
    "3. analyze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get the dataset and install NLTK\n",
    "First, we need to download our dataset and install the NLTK package.\n",
    "We will be using the book \"Frankenstein; Or, The Modern Prometheus\" by Mary Wollstonecraft Shelley from Project Gutenberg.\n",
    "This was the top downloaded book last week by a female author (as well as any author!).\n",
    "\n",
    "https://www.gutenberg.org/ebooks/84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset - only do this once\n",
    "!wget -O frankenstein.txt https://www.gutenberg.org/files/84/84-0.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK - only do this once\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Open and read the data\n",
    "\n",
    "Look at chapter 3 of the NLTK book for how to get text data including HTML, files, etc.\n",
    "http://www.nltk.org/book/ch03.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up, read, and close our frankenstein.txt file\n",
    "fh = open(\"frankenstein.txt\")\n",
    "raw_text = fh.read()\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean up the data\n",
    "\n",
    "Now we want to take our raw text and clean it up so that we are able to work with it. \n",
    "In this example we will clean up the data and only work with Chapters 1-4 as an example.\n",
    "Usually you do not want to process an entire text file (for example, in this file we see unnrelated text at the top of the file). So this will give us practice in cleaning the data as well. And will speed up processing by having fewer sentences to work with as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the raw text by line and cut text to Chapters 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of text entries. One entry per line.\n",
    "lines = raw_text.split(\"\\n\")\n",
    "\n",
    "# Print the hundredth line of the book\n",
    "print(lines[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the lines between line\n",
    "# Hint: I just looked at the line numbers in a text editor for simplicity.\n",
    "chapter_lines = lines[652:1516]\n",
    "\n",
    "print(chapter_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, chapters is not a string of text. It is a list of lines. So we will convert it back to a string of text using join. \n",
    "We will join lines together using a simple space \" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join our chapter_lines list into a single string of text called chapters\n",
    "# See example in Chapter 4 : http://www.nltk.org/book/ch04.html\n",
    "\n",
    "chapters = \" \".join(chapter_lines)\n",
    "\n",
    "# Check it out\n",
    "print(chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words\n",
    "Tokenization takes raw text and divides it up into tokens. There are a variety of ways of 'tokenizing' text. Two of the most common are words and sentences, but you can also do it by phrases and other characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(chapters)\n",
    "\n",
    "# Check it out. It is now a list of words.\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(chapters)\n",
    "\n",
    "# Check it out. It is now a list of sentences.\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual tokenization\n",
    "Sometimes you have to tokenize your text manually. Here we will tokenize by Chapters.\n",
    "This is custom programming that is not always supported so you have to write your own Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start splitting by chapters\n",
    "# I leave it as an exercise for you to work through the code.\n",
    "# Notice, for Chapter 1 it must be after the Chapter 1 split and before the Chapter 2 split.\n",
    "\n",
    "ch0s = chapters.split(\"Chapter 1\")\n",
    "ch1s = ch0s[1].split(\"Chapter 2\")\n",
    "ch2s = ch1s[1].split(\"Chapter 3\")\n",
    "ch3s = ch2s[1].split(\"Chapter 4\")\n",
    "\n",
    "# Now we have text for each chapter if we want!\n",
    "ch1 = ch1s[0]\n",
    "ch2 = ch2s[0]\n",
    "ch3 = ch3s[0]\n",
    "ch4 = ch4s[0]\n",
    "\n",
    "# Make a chapter tokens if we wish.\n",
    "chapter_tokens = [ch1, ch2, ch3, ch4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing \"Stop Words\"\n",
    "Stop words are commonly occurring words that rarely are the focus of NLP.\n",
    "Let's look at the most common words and then see if we cannot remove many of the ones that 'get in our way' such as: the, of, was, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the most common words using the FreqDist function.\n",
    "# See chapter 1: https://www.nltk.org/book/ch01.html\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "fdist = FreqDist(word_tokens)\n",
    "\n",
    "# Let's see frequency distribution of the 50 most common words in our text\n",
    "print(fdist.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side note: You can look at frequency of a single word such as genius\n",
    "print(fdist['genius'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the top 20 words and their counts\n",
    "fdist.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's remove these common stop words\n",
    "#\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# print(stop_words) # Uncomment if you want to check it out.\n",
    "\n",
    "# Create a clean_words list\n",
    "clean_words = []\n",
    "# Loop through each word, w, in word_tokens\n",
    "for w in word_tokens:\n",
    "    # If the word is not in the stop_words list, add it to our clean_words list using append\n",
    "    if w not in stop_words:\n",
    "        clean_words.append(w)\n",
    "\n",
    "# Check it out\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You try it\n",
    "Now plot the top 20 most frequently used words of our cleaned up dataset. Look at the code examples above for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note** You can create your own list of additional stop words to remove. This is usually necessary when you work in a specific area. I have applied NLP to a variety of areas and we always needed to create a customized list of words that occurred frequently in our data, but did not provide any useful insights. So they were continually added to our evolving list of words to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming\n",
    "We need to take irregular text (including pluralization and tenses) such as bicycle or bicycles are converted to base word bicycle. \n",
    "We will try to distil them down to their base word. There are two approaches to doing this: lemmatization and stemming.\n",
    "There are multiple stemmers and lemmatizers, here are two.\n",
    "\n",
    "Look here for an example: https://www.guru99.com/stemming-lemmatization-python-nltk.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer  = PorterStemmer()\n",
    "\n",
    "stem_words = []\n",
    "for w in clean_words:\n",
    "    stem_w = stemmer.stem(w)\n",
    "    stem_words.append(stem_w)\n",
    "\n",
    "# Check it out\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lem_words = []\n",
    "for w in clean_words:\n",
    "    lem_w = lemmatizer.lemmatize(w)\n",
    "    lem_words.append(lem_w)\n",
    "    \n",
    "# Check it out\n",
    "print(lem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the difference\n",
    "Take a look at the differences between stemming and lemmatizing words. One important difference that you cannot see if that of speed. Lemmatizing words is more complex so it is slower. If you are working with a lot of text (social media) sometimes you cannot lemmatize all of the text, because it is simply too slow. However, lemmatizing is generally considered superior, but there are caveats to that, which we will not get into today. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging Part of Speech (POS)\n",
    "We can tag our sentences and identify Part of Speech. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You try it!\n",
    "Go to Chapter 7 of the NLTK book here: http://www.nltk.org/book/ch07.html#fig-ie-architecture\n",
    "Look at the code example for post_tag and use the pos_tag function to create a new list of words with pos tagged.\n",
    "I would suggest using the lem_words list we just created above.\n",
    "Save the new pos list under a useful name such as pos_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging (you try it)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this statement to see what you just created. A list of words with their parts of speech.\n",
    "nltk.pos_tag([\"Hello\",\"world\",\"this\",\"is\",\"my\",\"statement\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ?! What is all this? Time for some reading\n",
    "Here is where NLP gets a little complex. You have to divde into grammar and syntax. Scan the first couple sections of Chapter 8 to get a sense. If text analytics could be an area of interest for you, then take some time to look over this chapter. Sometimes you need to dive into the mechanics of language for your problem. Other times you can avoid this entirely.\n",
    "http://www.nltk.org/book/ch08.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation\n",
    "Chapter 2 covers bigrams and how we can look at the frequency distribution of words. Read more here: http://www.nltk.org/book/ch02.html#chap-corpora\n",
    "\n",
    "Example code can be found in the collocations howto guide here:\n",
    "https://www.nltk.org/howto/collocations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Build a bigram finder using our word tokens\n",
    "finder = BigramCollocationFinder.from_words(word_tokens)\n",
    "\n",
    "# Get the top 10 most collocated words (those that are highly collocated).\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove collocated bigrams that occur fewer than 3 times\n",
    "finder.apply_freq_filter(5)\n",
    "\n",
    "# Now get the top 10 more commonly used collocated words\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
