{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Python Programming Workshop\n",
    "## Scrapy and 'borrowing' content from the web\n",
    "\n",
    "This notebook will introduce you to Scrapy, an open source and collaborative framework for extracting the data you need from websites (https://scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install scrapy\n",
    "This code cell is used to install the scrapy package using the python package management system called 'pip'\n",
    "\n",
    "**Note** You only need to run this once. After that, the package will be installed and you will not need to run this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A basic example\n",
    "Here we will create a basic example using Scrapy. We need to define a few terms. \n",
    " * A **spider** is a term used for the piece of software that 'crawls the web' meaning that this is the Python code that will grab the content we need from a website so that we can scrape it.\n",
    " * A **parse** function is the function that will run our scraping code.\n",
    " * A **Class** is a term used in object-oriented programming (OOP). Python is an object-oriented programming language. It represents a characterization of a thing like a 'dog' where you can give attributes such as a 'name' or functionality such as 'bark.' If you are interested in a deeper explanation you can visit the following page: [Python OOP](https://www.datacamp.com/community/tutorials/python-oop-tutorial).\n",
    " * A **URL** stands for Uniform Resource Locator, which is a web address.\n",
    " * The command **yield** will generate data that you are interested in scraping out of a website.\n",
    " * A **response** is the website (in the form of HTML code) that came back from our spider.\n",
    " \n",
    "This first code cell simply defines our spider. It does not run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "# This creates our own spider class called 'MySpider'\n",
    "class MySpider(scrapy.Spider):\n",
    "    # This names our spider. The name must be unique but doesn't matter much.\n",
    "    name = 'myspider'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        # This is the comma separated list of URLs you plan to scrape\n",
    "        start_urls = [\n",
    "            'https://en.wikipedia.org/wiki/Social_science',\n",
    "            'https://en.wikipedia.org/wiki/Geography'\n",
    "        ]\n",
    "        for start_url in start_urls:\n",
    "            yield scrapy.Request(url=start_url, callback=self.parse)\n",
    "        \n",
    "\n",
    "    # This is the function where we will scrape each website\n",
    "    def parse(self, response):\n",
    "        # First open our file for writing\n",
    "        # Get the page name (social_science or geography)\n",
    "        page = response.url.split(\"/\")[-1] \n",
    "        filename = f'my-scrapy-data-{page}.txt'\n",
    "        with open(filename, 'w') as f:\n",
    "            \n",
    "            # Now figure out what we are going to write\n",
    "            links = response.xpath(\"//a/@href\").getall()\n",
    "            for link in links:\n",
    "                f.write(link)\n",
    "            \n",
    "            #f.write(response.body)\n",
    "        self.log(f'Saved file {filename}')\n",
    "        \n",
    "        yield None # Just do nothing here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run our spider.\n",
    "\n",
    "Here we are importing a new module called CrawlerProcess, which will run our Spider. For more information see the following [webpage](https://docs.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script). We will also be importing the multiprocessing module, which allows us to reuse the crawler. The technical details for this are not important - you can read more here if you are interested: [webpage](https://wiseodd.github.io/techblog/2016/06/10/scrapy-long-running/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, hand waving time.\n",
    "# This code does a few advanced things to make the Scrapy crawler work inside of a notebook.\n",
    "# mp.Process launches a new computing process (that is separate from the notebook itself)\n",
    "# then you call the _getscraping function that you created inside that new process\n",
    "# That is where the crawler will call your parse function above\n",
    "\n",
    "# You don't need to understand this code to use scrapy. You just need to change the parse function above.\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import multiprocessing as mp\n",
    "\n",
    "def getscraping(spider):\n",
    "    def _getscraping(spider):\n",
    "        # Create a computer process that mimics a Mozilla Firefox browser.\n",
    "        process = CrawlerProcess({\n",
    "          'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "        })\n",
    "\n",
    "        # Run our 'MySpider' crawler. Warning you will see a lot of red info.\n",
    "        result = process.crawl(spider)\n",
    "        process.start()\n",
    "        return\n",
    "\n",
    "    # Create a multiprocessing queue to hold our result from our crawling\n",
    "    p = mp.Process(target=_getscraping, args=(spider,))\n",
    "    p.start()\n",
    "    p.join()\n",
    "    return None\n",
    "\n",
    "getscraping(MySpider)\n",
    "\n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful tip: You can turn off warnings (ignore) or only see them 'once'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
